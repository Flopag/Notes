# How to use for a lexer a non-deterministic finite automaton  representing a language? (Slides 2-22 -- 2-31)

Each token are defined using a regular expression. These regular expressions can be transformed into NFA.

To get the list of token, we can simulate all NFA at the same time. When all NFA fails, we re-insert the failed character on the reading stack and we consider the token corresponding to the last alive NFA (longest prefix matching). then, we redo the same process, until then end of the reading stack.

If there are multiple NFA that are the last alive one. Take the one that has the higher priority (arbitrary defined). 

if no NFA are able to accept the first character, an error is triggered. It can be done by making the NFA : $s_1 \rightarrow_\varepsilon s_{error}$. This NFA is always true for any string of size 1. So, if it is the last alive, that means that no other NFA accept the read character, and the error token will be consider.

In practice, all NFA are connected to a root and all accepting state correspond to a token. In this big NFA, we consider the token of the last accepted state reached.

The complexity depends on the number of states because we need to look after all possibilities at each new character analysed. For the worst case, there is always a number of possibilities equal to the number of states