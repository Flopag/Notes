# How is lexical analysis performed efficiently?Â  Assume you have a  certain amount of regular expressions describing tokens: how do you generate a lexer?

We can generate a lexer using NFA (see [Q2.3](Q2.3.md) for more details) :
1. Construct one NFA for each regular expression
2. Merge all these NFA into a unique NFA linking all starting state into a root state
3. For each guess, consider the token attached to the last accepted state of the NFA (longest prefix matching)

We ca, generate a lexer using NFA then DFA (see [Q2.5](Q2.5.md)):
1. Generate the lexer usign NFA (see above)
2. Convert the NFA into an DFA
3. Maybe minimise the number of state of the DFA
4. Same scanning as NFA (see above)
