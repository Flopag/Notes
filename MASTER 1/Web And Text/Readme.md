# Web and Text Analytics

- [Info_Retrieval_and_VSM](Slide_notes/Info_Retrieval_and_VSM.md)
- [TFIDF](Slide_notes/TFIDF.md)
- [text_classification](Slide_notes/text_classification.md)
- [RNN](Slide_notes/RNN.md)
- [Contextual_embeddings](Slide_notes/Contextual_embeddings.md)
- [Word_Embeddings](Slide_notes/Word_Embeddings.md)
- [Lecture_Intro_MT](Slide_notes/Lecture_Intro_MT.md)
- [MT_Evaluation](Slide_notes/MT_Evaluation.md)
- [Introduction_to_attention](Slide_notes/Introduction_to_attention.md)
- [Attention_Mechanism_in_NLP](Slide_notes/Attention_Mechanism_in_NLP.md)
- [LLMs](Slide_notes/LLMs.md)
- [Tranformers_from_scratch(LLaMa)](Slide_notes/Tranformers_from_scratch(LLaMa).md)

## Big summary (following [this](exam_content_notes.md))

### Document scoring

- [Jaccard index](Slide_notes/Info_Retrieval_and_VSM.md) : 
	- is overlapping of two sets : computed by counting the number of elements that appear in the two sets divided by the number of elements in the rest
	- Limitations : do not take occurrence and do not normalize documents length
- [Vector space model (VMS)](Slide_notes/Info_Retrieval_and_VSM.md) :
	- One vector per document
	- Count the number of occurrence of a word in the document (can be $0$)
	- Words are dimensions and their occurrence are the length of the vector in this dimension
- [Proximity Metric](Slide_notes/Info_Retrieval_and_VSM.md) : 
	- Measure the distance between two VMS
	- by Euclidean distance ($d_{euc} = \sqrt{\sum_{i=1}^n ( x_i - y_i )^2}$) : penalize large vectors
	- by angular distance ($\theta = \cos^{-1} {\frac {\sum_{i=1}^n x_iy_i}{||x|| \ ||y||}}$) : do not penalize large vectors, only look to relevant dimensions (words)
		- size of a vector is computed using $||x|| = \sqrt{\sum_{i=1}^n x_i^2}$

## [Term-Frequency Inverse-Document Frequency (TFIF)](Slide_notes/TFIDF.md)

- Term-frequency : the more mentions of a query is, the more relevant the document is
	- Documents with most words same as query have most values
	- Limitations : unimportant terms will grow the score (as "the")
- Inverse-Document Frequency : the less mentions of a query there is in all documents, the more relevant term is
	- Overall rare words have more values
- TFIF : combination of both
	- $TFIDF_{t, d} = tf_{t,d} \times \log \frac{N}{df_t}$
	- Where :
		- $tf_{t,d}$ is the number of occurrence of term $t$ in document $d$ (term frequency)
		- $df_t$ is the number of occurrence of term $t$ in all documents
		- $N$ is the number of documents of the collection

## [Text classification - Features selection](Slide_notes/text_classification.md)

- is the process of selecting a subset of relevant features from the full set of features of the dataset
	- Feature = value given as input
- Two types of methods :
	- Linguistic methods : Identify features based on linguistic information (e.g. grammar)
	- Statical methods : Identify features based on the occurrence frequency
		- TFIDF (see above)
		- Mutual information : the amount of information that the term contains about the class (see [here](Slide_notes/text_classification.md) to get formula)
		- Chi-square : the amount of dependence between events (see [here](Slide_notes/text_classification.md) to get formula)
			- big values is big dependence
		- Multinomial Naïve Bayes : predict the best class given a document
			- It is naïve because it makes two assumptions to facilitate probability computations :
				- Each text document is generated by a mixture model
					- A mixture model models data using several statistical distributions
					- So, documents are generated using statistical distributions
				- There is a one to one mapping between each mixture component and the document class
					- A mixture component is a distribution used in the mixture model
					- So, each class is associated to a statistical distribution
			- it is good for text classification because it do good comparisons between class despite that it does deliver un accurate solo values
				- Good because robust, efficient and easy

## [Recurrent Neural Network (RNN)](Slide_notes/RNN.md)

- are neural networks where the neurones are trained using backpropagation trough time and process sequential data (RNN remembers information, basic NN don't)
- Three types of layer : Input, Hidden, Output
- Propagation of states : $h_t = f_W(h_{t-1}, x_t)$
	- for basic RNN, use of : $f_W(h_{t-1}, x_t) = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$
- Output : $y_t = W_{hy}h_t$
	- Many to One : output one value at the end of the sequence of inputs
	- One to Many : output multiple value for one input
	- Many to many : output multiple values for multiple inputs
		- output for each input
		- output time not related to input time
- Output is used to compute the loss and then back propagate
- Backpropagation trough time (BPTT) : back propagation but trough states (time) instead of only trough layers
	- Each back propagation, the weights are updated doing a sort of memory
- Sequence to Sequence architecture (Seq2Sec)
	- Used in machine translation
	- Combination of encoder-decoder, decoder gets output of encoder as input
	- Encoder : 
		- Many to one
		- Output the context vector from input (if use of [softmax](Slide_notes/Word_Embeddings.md), it becomes a probability distribution)
	- Decoder :
		- One to many
		- Output the sequence given the context vector from encoder
- Vanishing gradient : The gradient can vanish trough time due to backpropagation
	- Back propagation can divide some value until vanishing
	- The inverse is called exploding gradient
	- Can be fixed using : LSTM or GRU
		- LSTM : introduction of memory cells and gates that control the flow of information
		- Four gates : control how much information is passed through or removed from the memory cell. They consist of sigmoid (output between 0 and 1)
			- Forget gate : how much information from the previous cell state to forget
			- Input gate : how much new information to add to the memory cell
			- Output gate : how much of the memory cell to pass to the output
		- Prevent vanishing gradient tanks to the only use of the Forget gates that have a value between 0 and 1, smoother gradient flow

## [Machine translation evaluation](Slide_notes/MT_Evaluation.md)

- Evaluation aim to determine how good is a machine translation system
- Two kinds of translation quality :
	- Adequacy : is the meaning kept ?
	- Fluency : is the grammar correct ?
- Human evaluation : not so good because a lot of subjectivity
- Precision-Recall : look at words that are exactly the same than in reference
	- $F1$-score computed making the proportion of word in both text
	- Bad, do not look at the meaning, only words is important not their position
- Word Error Rate (WER) : look at the number of steps needed to go from output text to reference
	- Each type of steps (substitution, insertion, deletion) have a fixed value
	- This can be computed by finding the less costly path between both texts
- BLEU : look at clusters similitude between output text and reference
	- $BLEU_4 = \min\left(1, \frac{output length}{reference length}\right) \left(\prod_{i=1}^4 precision_i\right)^{\frac 1 4}$
	- BLEU-1 is the same as precision-recall because the cluster size are of one word
	- Limitation : it overlook the semantics and the meaning can be fault. (see [example](Slide_notes/MT_Evaluation.md))

## [Attention mechanism](Slide_notes/Introduction_to_attention.md)

- Attention mechanism permit neural network to focus on some information
- Two types of attention :
	- Natural : incidence of the neural network itself (e.g. RNN)
		- We do not have control on it
	- Implicit : attention is made intentionally (forced)
- To analyse attention, we use a matrix that models the relationship between inputs and outputs, the Jacobian :
	- The elements of the Jacobian are $J_{ij} = \frac{\partial \ y_i}{\partial \ x_j}$
	- This matrix is computed by backpropagating the input directly (not its loss)
	- RNN have a time dimension, so, the elements become $J_k^t = \left(\frac{\partial y_k^t}{\partial \vec x_1}, \frac{\partial y_k^t}{\partial \vec x_2}, ...\right)$
		- Can lead to complicated computation because elements can be widely spread
- Soft attention : todo

## [Contextual embeddings](Slide_notes/Contextual_embeddings.md)

- An embedding is the representation of the meaning of a word
- Two types of embedding :
	- Static embedding : pre-trained, represent the embedding of the pretrained phase
	- Dynamic embedding : can change and influence each other, represents the context of the pretrained phase and the context of the current data
- Contextual embedding is dynamic embedding since each word embedding depends of their surrounding words embeddings
	- Bi-LSTM : mixing left and right static embeddings. 
		- The more the embeddings are far away, the less impact they have on each other
	- ELMo : add a Bi-LSTM output as input, sided with the true input
	- BERT : use bidirectional transformer, so, process input in parallel
		- The input of the transformers is a mix of WordPiece and Positional embedding :
			- WordPiece embeddings : Tokenisation made word by word
			- Positional embedding : encode position using $sin$ and $cos$
				- Permit to give position to transformers, because transformers forget positional data (du to parallelism)

## [Word embeddings](Slide_notes/Word_Embeddings.md)

- Word embeddings is the embedding of a word
- It often represent the context of the linked token
- Word2Vec : use of a model to make the context vector
	- Can be done using two algorithm :
		- CBOW
		- SG
	- Continuous bag of Words (CBOW) predict target from context
		- Aggregates the embeddings of the words around the target to predict it
		- Pros :
			- Faster to train
			- Suitable for large datasets with frequent words
	- SG predict context from target
		- Get the target embedding and predict words that are likely next to it
		- Pros :
			- Handle rare words
			- Capture more specific word relationship

## [LLM](Slide_notes/Tranformers_from_scratch(LLaMa).md)

- LLM are often based on transformers with stack of encoder and decoder
- Positional embedding :  gives information about the position (useful for transformers)
- Relative positional embedding :  gives information about the proximity between two words
	- Hard to compute 
	- Must have fixed positional embedding
- Rotary positional embedding (RoPE) : gives information of position through rotations
	- Permit to get relative position, as angle do not change if we modify all but distance
- KV cache : store data during inference
	- Avoid useless re-computation