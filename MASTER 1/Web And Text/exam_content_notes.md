----
Web_and_text_analytics:

1.4: document scoring
skip algebra to cosing (we study cosing, we must know how to compute and why it is important)
1.7: importnat
1.8: important

------
Process documents

white turn frequency
tfidf (1.1.3) (remember formula)


Text classification:
fieter election
naive biased

feature selection methods (how much does an occurence of a word anable us to predict the class ????????)
mutual infirmation (complex formula but izi to use)

1.4.1.2

multimonial naive bayes (why it is concider to be a naive ...)

start 1.5.2 exept (why it is difficult to compute) continue until the work example and until the end

Skip how to expend the ... the rest we can learn (?)

-------
language model:
skip 1.1 and the rest

------
RNN:
learn everything

do not remember the formula but must be able to explain
whitch non linear function is used for witch gates, explain how the lstm problem ...

study until slide 48
--------
machine learning part 2

evaluation, why presition recall are not usefull for machine translation
how do you compute the bleu score

skip wern...

limitation of bleu is very important

----
attention mechanism in NLP:

what is is realy decoding and the limitation

two type of nodes:
explicite attention mechanism

17 until 23

-----
Word embeddings:

what is the adventage of both word2vec methods (scippgraan and the other) (i thing, not sure)

----
contectual embeding

differnce between contextual embeding and static embeding

from 9 until 32

-----
llama:

focus diff etween lama and transformer

what is normilization, batch and layer, RMSnorm

positional embeddings (we stick on the standard positional embeding used in transformers)

everything until 

Kv cach

(all seems important)

unitl 53 then we can skip (?)