# Q11 : Support vector machines

> **Support vector machines**: main notions (classification margin, optimization problem, support vectors, soft margin), general idea of the kernel trick.

## Main notions

### Classification margin

The margin is the width that the boundary could be increased by before hitting a data-point

We search the linear classifier with the biggest margin

![](attachments/Pasted%20image%2020231228135121.png)

### Optimisation problem

The optimisation problem is:

$$arg \max_{w, w_0}{\left\{\frac 1 {||w||} \min_n {[y_n (w^Tx_n+w_0)]}\right\}}$$

An equivalent is $\min_{w, w_0} \varepsilon(w, w_0)=\frac 1 2 ||w||^2$ according to the constraint $y_k(w^Tx_k+w_0)\geq 1  \ \ \ , \forall k=1,...,N$

Hard to understand, must be done if time

### Support vectors

Hard to understand, must be done if time

### Soft margin

Due to noise or outliers, the samples may not be linearly separable in the feature space

Discrepancies with respect to the margin is measured by slack variables $\xi_i \geq 0$ with the associated relaxed constraints $y_i(w^Tx_i+w_0)\geq 1-\xi_i$

By making $\xi_i$ large enough, the constraints can always be met:
- if $0<\xi_i<1$ the margin is not satisfied but $x_i$ is still correctly classified
- if $\xi_i > 1$ then $x_i$ is misclassified

![](attachments/Pasted%20image%2020231228141032.png)

Primal problem:

$$\min_{w, \xi}{\frac 1 2 ||w||^2+c\sum_{i=1}^N\xi_i}$$
$$subject \ to \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$
$$ y_i(w^Tx_i+w_0)\geq 1 - \xi_i, \xi_i \geq 0, \forall i=1,...,N$$
Where $c$ is a positive constant balancing the objective of maximizing the margin and minimizing the margin error

Dual problem:

$$\max_\alpha W(\alpha) = \sum_{k=1}^N{\alpha_k}-\frac 1 2 \sum_{i, j=1}^N{\alpha_i\alpha_jy_iy_jx_i^Tx_j}$$

$$subject \ to \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

$$0 \leq \alpha_k \leq c, \forall k=1,...,N$$

$$\sum_{i=1}^N\alpha_iy_i=0$$

![](attachments/Pasted%20image%2020231228142057.png)

### Pros and Cons

Pros:
- State-of-the-art accuracy on many problems
- Can handle any data types by changing the kernel

Cons:
- Tuning the method parameter is very crucial to get good results and somewhat tricky
- Black-box models, not easy to interpret

## Kernel trick

for non linear boundary, we can remap the data into a new feature space where the boundary is linear

![](attachments/Pasted%20image%2020231226101309.png)

Lets consider a non-linear mapping $\phi$ to a new feature space $\phi(x)=[z_1, z_2, z_3]^T = [x^2_1, x_2^2, \sqrt 2 x_1 x_2]^T$

the dual problem becomes:

$$\max_\alpha W(\alpha) = \sum_{k=1}^N{\alpha_k}-\frac 1 2 \sum_{i, j=1}^N{\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)}$$

Instead of explicitly defining a mapping $\phi$, we can directly specify the dot product $\phi^T(x)\phi(x')$ and leave the mapping implicit

The kernel trick consist in replacing this dot product by a kernel:

$$\phi^T(x)\phi(x') = K(x,x')$$

