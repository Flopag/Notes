# Q6 : Classification and regression trees

> **Classification and regression trees**: hypothesis space, growing and pruning algorithms, impurity measures, extensions to regression and continuous attributes, interpretability.

## Tree learning

A classification trees is a supervised learning algorithm that can handle classification problem and discrete or continuous attribute

Classification trees have a class on each leaf 

![](attachments/Pasted%20image%2020231005171207.png)

Regression trees have a number on each leaf

![](attachments/Pasted%20image%2020231005171147.png)

classification trees are represented with decision trees where :
- Each interior node tests an attribute
- Each branch corresponds to an attribute value
- Each leaf is labelled with a class

### Hypothesis space

?

### Growing algorithms

- construct a decision tree that has one path to a leaf for each example.
	- Problem: it does not capture useful information from the database.
- generate all trees and pick the simplest one that is consistent with the learning sample.
	- Problem: there are too many trees.
- Top-down induction : Choose the best attribute, split the learning sample accordingly and proceed recursively until each object is correctly classified. The best attribute is the one that make its successors as pure as possible (maximize the expected reduction of impurity)
	- Highly dependent upon the criterion for selecting attributes to test
	- It is sub-optimal because of the heuristic but very fast

![](attachments/Pasted%20image%2020231005110349.png)

### pruning algorithms

Pruning algorithms permit to avoid overfitting

There is two methods :
- **Pre-pruning** : stop growing the tree earlier, before it reaches the point where it perfectly classifies the learning sample.
	- Idea : stop splitting a node if either :
		- The LS size is $< N_{min}$
		- The LS impurity is $<I_{th}$
		- The impurity reduction $\Delta I$ of the best test is not large enough
- **Post-pruning** : allow the tree to overfit, then post-prune it.
	1. Split the learning sample into twp sets
		- A growing sample $GS$ to build the tree
		- A validation sample $VS$ to evaluate its generalization error
	2. Compute a sequence of trees $\{T_1, T_2,...\}$ where:
		- $T_1$ is the complete tree
		- $T_i$ is obtained by removing some test nodes from $T_{i-1}$, two methods :
			- Reduced error pruning : at each step, remove the node that most decreases the error on the validation sample
			- Cost-complexity pruning : define a cost-complexity criterion : $Error_{GS}+\beta \ Complexity(T)$ and build the sequence of trees that minimizes this criterion, for increasing value of $\beta$
	3. Select the tree $T^*_i$ from the sequence that minimizes the error on $VS$
	- For small data sets
		1. Grow a tree from the whole database
		2. post-prune it by 10-fold cross-validation
		3. Estimate its accuracy by 10-fold cross-validation

### Impurity measures

Let assume that $LS$ is the learning sample and $p_j$ the proportion of objects in the $LS$ belonging to output-class $j$

An impurity measure $I(LS)$ should satisfy the following props :
- $I(LS)$ is minimum only when $\exists i$ such that $p_i=1$ for $j\ne i$ (pure sample)
- $I(LS)$ is maximum only when $\forall j$ : $p_j=\frac 1 J$ (uniform number of object among classes)
- $I(LS)$ is a symmetric function of its arguments $p_1,...,p_J$

There is multiple impurity measure :
- Shannon entropy : $I_{Sh}(LS) = -\sum_{j=1}^J {p_j \ log_2 \ p_j}$ (green)
- Gini index : $I_{Gi}(LS)=\sum_{j=1}^J{p_j(1-p_j)}$ (blue)
- Error rate : $I_{ER}(LS) = 1 - \max_j {p_j}$ (red)

![](attachments/Pasted%20image%2020231005111756.png)

Reduction of impurity :

$$\Delta I(LS,A)=I(LS)-\sum_{a\in A(LS)}{\frac{|LS_a|}{|LS|}I(LS_a)}$$

Where : 
- $LS_a$ is the subset of object $o$ from LS such that $A(o) = a$
- $A(LS)$ is the set of different value of $A$ observed in $LS$
- $|LS_a|$ is the size of $LA_a$

ex :

![](attachments/Pasted%20image%2020231005113702.png)

#### Regression trees

The impurity of a sample is defined by the variance of the output in that sample :

$$I(LS) = var_{y|LS}\{y\}=E_{y|LS}\{(y-E_{y|LS}\{y\})^2\}$$

The best split is the one that reduces the most variance :

$$\Delta I(LS, A)=var_{y|LS}\{y\}-\sum_a{\frac{|LS_a|}{|LS|}var_{y|LS_a}\{y\}}$$

### Extensions to regression and continuous attributes

To handle continuous attributes, there are two solutions :
- Pre-discretize before tree growing
- Discretize during tree growing

How to descretize (find a threshold) ?
1. Sort the attribute that we want to descretize
2. Calculate $\Delta I$ for each different values
3. Take the value that has the biggest $\Delta I$ for the threshold

![](attachments/Pasted%20image%2020231005170434.png)

To handle attributes with many values, there are two solutions :
- Change the splitting criterion to penalize attributes with many values
	- Problem : the gain ration favours unbalanced tests
- Consider only binary splits (better)

![](attachments/Pasted%20image%2020231005170753.png)

To handle missing attributes, there are three solutions :
- Assign the most common value in the learning sample
- Assign the most common value in the tree
- Assign a probability to each possible value

### Interpretability

With decision trees, interpretability is obvious