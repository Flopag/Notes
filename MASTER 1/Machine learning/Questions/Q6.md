# Q6 : Classification and regression trees

> **Classification and regression trees**: hypothesis space, growing and pruning algorithms, impurity measures, extensions to regression and continuous attributes, interpretability.

## Tree learning

A classification trees is a supervised learning algorithm that can handle classification problem and discrete (classification) or continuous (regression) attribute

classification trees are represented with decision trees where :
- Each interior node tests an attribute
- Each branch corresponds to an attribute value
- Each leaf is labelled with a class

### Hypothesis space

### Growing algorithms

- construct a decision tree that has one path to a leaf for each example.
	- Problem: it does not capture useful information from the database.
- generate all trees and pick the simplest one that is consistent with the learning sample.
	- Problem: there are too many trees.
- Top-down induction : Choose the best attribute, split the learning sample accordingly and proceed recursively until each object is correctly classified. The best attribute is the one that make its successors as pure as possible (maximize the expected reduction of impurity)
	- Highly dependent upon the criterion for selecting attributes to test
	- It is sub-optimal because of the heuristic but very fast

![](attachments/Pasted%20image%2020231005110349.png)

### pruning algorithms

Pruning algorithms permit to avoid overfitting

### Impurity measures

Let assume that $LS$ is the learning sample and $p_j$ the proportion of objects in the $LS$ belonging to output-class $j$

An impurity measure $I(LS)$ should satisfy the following props :
- $I(LS)$ is minimum only when $\exists i$ such that $p_i=1$ for $j\ne i$ (pure sample)
- $I(LS)$ is maximum only when $\forall j$ : $p_j=\frac 1 J$ (uniform number of object among classes)
- $I(LS)$ is a symmetric function of its arguments $p_1,...,p_J$

There is multiple impurity measure :
- Shannon entropy : $I_{Sh}(LS) = -\sum_{j=1}^J {p_j \ log_2 \ p_j}$ (green)
- Gini index : $I_{Gi}(LS)=\sum_{j=1}^J{p_j(1-p_j)}$ (blue)
- Error rate : $I_{ER}(LS) = 1 - \max_j {p_j}$ (red)

![](attachments/Pasted%20image%2020231005111756.png)

Reduction of impurity :

$$\Delta I(LS,A)=I(LS)-\sum_{a\in A(LS)}{\frac{|LS_a|}{|LS|}I(LS_a)}$$

Where : 
- $LS_a$ is the subset of object $o$ from LS such that $A(o) = a$
- $A(LS)$ is the set of different value of $A$ observed in $LS$
- $|LS_a|$ is the size of $LA_a$

ex :

![](attachments/Pasted%20image%2020231005113702.png)

### Extensions to regression and continuous attributes

### Interpretability