**Clustering methods**# Q15 : Clustering methods

> **Clustering methods**: problem definition and motivation, hierarchical clustering, k-means, determination of the number of clusters.

## Problem definition

Try to find natural groups of sample/variables by grouping a collection of objects into subset (cluster) such that those within each cluster are more closely related to on another than to objects from different clusters

Clustering:
- Rows: grouping similar objects
- Columns: grouping similar variables across sample
- Two-way: grouping object that are similar across a subset of variables

Two components are essential in cluster analysis:
- Distance measure: notion of distance/similarity of two objects
- Cluster algorithm: procedure to minimize distance of object within groups and/or maximize distance between groups

![](attachments/Pasted%20image%2020231226104715.png)

### Distance between clusters

- Single linkage: use the smaller distance $d_S(G,H)=\min_{i\in G,j\in H} d_{ij}$
- Complete linkage: use the largest distance $d_C(G,H)=\max_{i\in G,j\in H} d_{ij}$
- Average linkage: use the average distance $d_A(G,H)=\frac 1 {N_GN_H}\sum_{i\in G}\sum_{j\in H} d_{ij}$

![](attachments/Pasted%20image%2020231229144525.png)

## Motivation

Unsupervised learning tries to find any regularities in the data without guidance about inputs and outputs. We want to find interesting groups and dependency between variables

## Hierarchical clustering

Agglomerative clustering method:
1. Each object is assigned to its own cluster
2. Iteratively:
	1. The two most similar clusters are joined and replaced by a new one
	2. The distance matrix $d$ between clusters is updated with this new cluster, replacing the two joined ones

A good representation is the dendrograms. Clusters that are joined are combined by a line. the height of a the line correspond to the distance between these clusters

![](attachments/Pasted%20image%2020231229144825.png)

Pros:
- No need to assume any particular number of clusters
- Can use any distance metric
- Can sometimes find a meaningful taxonomy
Cons:
- Can find a taxonomy even if it does not exist
- Once a decision is made to combine two clustersn it cannot be undone
- Not well motivated from a theoritical point of view

## K-means

is a partitioning algorithm with a prefixed number of $k$ clusters. It uses the euclidean distance:

$$d(x_i,x_{i'})=\sum_{j=1}^p (x_{ij}-x_{i'j})^2=||x_i-x_{i'}||^2$$
Algorithm:
1. Randomly assign each point to a cluster
2. Iterate:
	1. Given the current cluster assignment, compute the cluster means $\{m_1,...,m_K\}$
	2. Given the current cluster means, assign each observation to the closest cluster mean $C(i)=arg \min_{1\leq k\leq K}||x_i-m_k||^2$
3. Stop when the assignment does not change anymore

Convergence is ensured but towards a local optimum only. The result varies with the initial state. Solution: restart the algorithm several times and pick the clustering that minimizes $W(C)$

Pros:
- Simple and understandable
- Can cluster any new point
- Well motivated theoretically
Cons:
- Must fix the number of cluster beforehand
- Sensitive to the initial choice of cluster centers
- Sensitive to outliers


![](attachments/Pasted%20image%2020231229145917.png)

![](attachments/Pasted%20image%2020231229145924.png)

![](attachments/Pasted%20image%2020231229145933.png)

## Determination of the number of clusters

it is similar to overfitting in supervised learning:
- Too many cluster can overfit the data
- Too few cluster can underfit the data

We cannot cross-validate

How can we choose a number of clusters?
- Locate the knee in the intra-cluster variance curve

![](attachments/Pasted%20image%2020231229150532.png)

- Internal indices
- Gap statistic
- Stability