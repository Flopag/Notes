# Q10 : Neural networks

> **Neural networks**: multilayer perceptrons (definition, representation capacity, back-propagation, techniques to avoid overfitting)

## Definition

A multi layer perceptrons is multiple layers of neurons, with each layer fully connected to the next

Notations :
- $L$ number of layers
- $s_l$ $(1\leq l \leq L)$ is the number of neurons in layer $l$
- $a_i^{(l)}$ $(1< l \leq L, 1 \leq i \leq s_l)$ is the activation (output) of the neuron $i$ of the layer $l$ for an object $o$
- $f^{(l)}$ $(2\le l \leq L)$ is the activation function of layer $l$
- $w_{i,j}^{(l)}$ $(1\leq i \leq s_{l+1}, 1 \leq j \leq s_l)$ is the weight of the edge from neuron $j$ in layer $l$ to neuron $i$ in layer $l+1$
- $w_{i,0}^{(l)}$ $(1\leq i \leq s_{l+1})$ is the bias/intercept of neuron $i$ in layer $l+1$

Prediction (recursively) :

$$a_i^{(1)}(o) = a_i (o)$$

$$a_i^{(l+1)}(o)=f^{(l+1)}(w_{i,0}^{(l)}+\sum_{j=1}^{s_l}{w_{i,j}^{(l)}a_j^{(l)}(o)})$$

![](attachments/Pasted%20image%2020231022175842.png)

### Pros and Cons

Pros:
- Universal approximation
- May be very accurate

Cons:
- The learning phase may be very slow
- Black-box models, very difficult to interpret
- Scalability

## Representation capacity

I don't understand

### General principles of the training algorithm

Main idea : finding the parameters $W$ that minimises the average loss over the training data :

$$W^* = \arg \max_W{\frac 1 N \sum_{o\in LS}{L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}}$$

with :
- $W$ the weights of all network
- $L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$ the loss function that compare the output layer prediction ($\mathbf g()$) and the true outputs ($\mathbf y()$)

We use the gradient descent to iteratively improve an initial value of $W$ :

$$\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$$

The gradient can be computed using back-propagation:
1. Choose a network structure and a loss function $L$
2. Initialize all network weights $w_{i,j}^{(l)}$ appropriately (typically: small random weights)
3. repeat until stopping criterion is met:
	1. Using back-propagation, compute either:
		- batch mode: $\Delta w_{i,j}^{(l)} = \frac 1 N \sum_{i\in LS}{\frac {L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}{\partial w_{i,j}^{(l)}}}$
		- online mode: $\Delta w_{i,j}^{(l)} = \frac {L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}{\partial w_{i,j}^{(l)}}$
	2. Update the weights according to: $w_{i,j}^{(l)} \leftarrow w_{i,j}^{(l)} - \eta \Delta w_{i,j}^{(l)}$ where $\eta \in ]0, 1]$ is the learning rate

Mini-batch mode: compute the gradient on small subsets of $q$ objects
## back-propagation

Back propagation is an algorithm that compute the gradient 

Lets consider the case of a single regression output square error and that all activation functions are similar:

$$L(\mathbf g (\mathbf a (o);W),\mathbf y (o)) = \frac 1 2 (g(\mathbf a (o);W)-y(o))^2 = \frac 1 2 (a_1^{(L)}(o)-y(o))^2$$

$z_i^{(l)}(o)$ is the values sent through the activation functions (input of the neuron) :

$$z_i^{(l)}(o)=w_{i,0}^{(l-1)}+\sum_{j=1}^{s_l-1}{w_{i,j}^{(l-1)}a_j^{(l-1)}(o)}$$

We use partial derivative to derivate $L$ by $a_i^{(l+1)}$:

$$\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o)) = \frac {\partial L(...)}{\partial a_i^{(l+1)}}\frac {\partial a_i^{(l+1)}}{\partial z_i^{(l+1)}}\frac {\partial z_i^{(l+1)}}{\partial w_{i,j}^{(l)}} = \frac {\partial L(...)}{\partial a_i^{(l+1)}} f'(z_i^{(l+1)})a_j^{(l)}$$

(Some more demonstration, but i don't have time to write it. The results are in the algorithm)

Algorithm become:
1. Compute $a_i^{(l)}(o)$ and $z_i^{(l)}(o)$ for all neurons (forward propagation)
	1. $a_i^{(l+1)}(o)=f^{(l+1)}(w_{i,0}^{(l)}+\sum_{j=1}^{s_l}{w_{i,j}^{(l)}a_j^{(l)}(o)})$
	2. $z_i^{(l)}(o)=w_{i,0}^{(l-1)}+\sum_{j=1}^{s_l-1}{w_{i,j}^{(l-1)}a_j^{(l-1)}(o)}$
2. Compute $\delta_i^{(l)}(o)$ for all neurons (backward propagation)
	1. $\delta_1^{(L)}(o)=\left(a_1^{(L)}(o)-y(o)\right)f'\left(z_1^{(L)}(o)\right)$
	2. $\delta_i^{(l)}(o)=\left(\sum_{j=1}^{s_l+1}{\delta_k^{(l+1)}(o)w_{j,i}^{(l)}}\right)f'\left(z_i^{(l)}(o)\right)$
3. Compute
	1. $\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o)) = \delta_i^{(l+1)}(o)a_j^{(l)}(o)$

## Techniques to avoid over-fitting

- Early stopping : stop gradient descent iterations before convergence by controlling the error on an independent validation set
	- If initial weights are small, the more iterations, the more non-linear become the model

![](attachments/Pasted%20image%2020231227150530.png)

- Weight decay : add an extra-term to the loss function that penalise too large weights
	- $W^* = \arg \max_W{\frac 1 N \sum_{o\in LS}{L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}} + \lambda \frac 1 2 \sum_{l=1}^{L-1}\sum_{i=1}^{S_{l+1}}\sum_{j=1}^{s_l}(w_{i,j}^{(l)})^2$
	- $\lambda$ controls complexity and be turned on a validation set
	- weight decay : give a penalty to big weight (to keep all weight small)
- Dropout : Randomly drop neurons from each layer with probability $\Phi$ and train only the remaining ones
	- That make the learned weights of a node more insensitive to the weights of the other nodes
	- This forces the network to learn several independent representations of the patterns and thus decreases over-fitting
- Unsupervised pretraining : train each hidden layer in turn in an unsupervised way, so that it allows to reproduce the input of the previous layer. and, introduce the output layer and then fine-tune the whole system using backpropagation