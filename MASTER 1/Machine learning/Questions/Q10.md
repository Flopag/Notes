# Q10 : Neural networks

> **Neural networks**: multilayer perceptrons (definition, representation capacity, back-propagation, techniques to avoid overfitting)

## Definition

A multi layer perceptrons is multiple layers of neurons, with each layer fully connected to the next

Notations :
- $L$ number of layers
- $s_l$ $(1\leq l \leq L)$ is the number of neurons in layer $l$
- $a_i^{(l)}$ $(1< l \leq L, 1 \leq i \leq s_l)$ is the activation (output) of the neuron $i$ of the layer $l$ for an object $o$
- $f^{(l)}$ $(2\le l \leq L)$ is the activation function of layer $l$
- $w_{i,j}^{(l)}$ $(1\leq i \leq s_{l+1}, 1 \leq j \leq s_l)$ is the weight of the edge from neuron $j$ in layer $l$ to neuron $i$ in layer $l+1$
- $w_{i,0}^{(l)}$ $(1\leq i \leq s_{l+1})$ is the bias/intercept of neuron $i$ in layer $l+1$

Prediction (recursively) :

$$a_i^{(1)}(o) = a_i (o)$$

$$a_i^{(l+1)}(o)=f^{(l+1)}(w_{i,0}^{(l)}+\sum_{j=1}^{s_l}{w_{i,j}^{(l)}a_j^{(l)}(o)})$$

![](attachments/Pasted%20image%2020231022175842.png)

## Representation capacity

/

### General principles of the training algorithm

Main idea : finding the parameters $W$ that minimises the average loss over the training data :

$$W^* = \arg \max_W{\frac 1 N \sum_{o\in LS}{L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}}$$

with :
- $W$ the weights of all network
- $L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$ the loss function that compare the output layer prediction ($\mathbf g()$) and the true outputs ($\mathbf y()$)

We use the gradient descent to iteratively improve an initial value of $W$ :

$$\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$$

## back-propagation

Back propagation is an algorithm that compute the gradient :
1. Compute $a_i^{(l)}(o)$ and $z_i^{(l)}(o)$ for all neurons (forward propagation)
	1. $a_i^{(l+1)}(o)=f^{(l+1)}(w_{i,0}^{(l)}+\sum_{j=1}^{s_l}{w_{i,j}^{(l)}a_j^{(l)}(o)})$
	2. $z_i^{(l)}(o)=w_{i,0}^{(l-1)}+\sum_{j=1}^{s_l-1}{w_{i,j}^{(l-1)}a_j^{(l-1)}(o)}$
2. Compute $\delta_i^{(l)}(o)$ for all neurons (backward propagation)
	1. $\delta_1^{(l)}(o)=\left(a_1^{(L)}(o)-y(o)\right)f'\left(z_1^{(L)}(o)\right)$
	2. $\delta_i^{(l)}(o)=\left(\sum_{j=1}^{s_l+1}{\delta_k^{(l+1)}(o)w_{j,i}^{(l)}}\right)f'\left(z_i^{(l)}(o)\right)$
3. Compute
	1. $\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o)) = \delta_i^{(l+1)}(o)a_j^{(l)}(o)$

## Techniques to avoid over-fitting

- Early stopping : stop gradient descent iterations before convergence by controlling the error on an independent validation set
- Weight decay : add an extra-term to the loss function that penalise too large weights
	- $W^* = \arg \max_W{\frac 1 N \sum_{o\in LS}{L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}} + \lambda \frac 1 2 \sum_{l=1}^{L-1}\sum_{i=1}^{S_{l+1}}\sum_{j=1}^{s_l}(w_{i,j}^{(l)})^2$
	- $\lambda$ controls complexity and be turned on a validation set
	- weight decay : give a penalty to big weight (to keep all weight small)
- Dropout : Randomly drop neurons from each layer with probability $\Phi$ and train only the remaining ones
	- That make the learned weights of a node more insensitive to the weights of the other nodes
	- This forces the network to learn several independent representations of the patterns and thus decreases over-fitting
- Unsupervised pretraining : train each hidden layer in turn in an unsupervised way, so that it allows to reproduce the input of the previous layer. and, introduce the output layer and then fine-tune the whole system using backpropagation