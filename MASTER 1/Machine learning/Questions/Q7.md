# Q7 : Linear regression

> **Linear regression**: problem formulation, analytical solution, introduction of a regularization term (motivation and modified solution), linear regression with kernels.

## Problem formulation

The learning sample (LS) is composed of observations $o_i$

observation $o_i$ is composed of inputs $a_j(o_i) = a_j^i$ and an output $y^i$
- $\forall i = 1, ..., N$
- $\forall j = 1,...,n$

So, we can model LS with two matrix :
- The matrix of inputs $\mathbf A=a_j^i$
- The matrix of outputs $\mathbf Y = y^i$

![](attachments/Pasted%20image%2020231015165312.png)

Linear regression tries to approximate outputs by using :

$$\hat y(o) = w_0+\sum_{i=0}^n{w_i a_i(o)}$$

Must choose parameters $w_0, w_1, ..., w_n$ to fit well (supervised learning)

The parameters are linear but the original inputs can be non linear. We can handle this by changing the space of the original inputs to a space where their are linear:

$$\hat y(o) = w_0+\sum_{i=0}^n{w_i \phi_i(\mathbf a(o))}$$

### Pros and cons

Pros:
- Simple
- There exist fast and scalable variants
- Provide interpretable models through variable weight

Cons:
- Often not as accurate as other (non-linear) methods

## Analytical solution

### Least mean square error solution

Principle : we look for $\mathbf w'$ that minimise $TSE(LS,\mathbf w')$ 

We pose $a_0(o)=1$ :
- $\mathbf a'(o_j) = (a_0(o_j),a_1(o_j) , ..., a_n(o_j))^T$
- $\mathbf w'=(w_0, w_1, ..., w_n)^T$
- $\mathbf A'=(\mathbf a'(o_1), \mathbf a'(o_2), ..., \mathbf a'(o_N))$

The squared error is :

$$SE(o_i, \mathbf w')=(y(o_i)-\hat y(o_i))^2 = (y(o_i) - \mathbf w'^T \mathbf a'(o_i))^2$$

The total squared error is :

$$TSE(LS,\mathbf w')= \sum_{i=1}^N{(y(o_i) - \mathbf w'^T \mathbf a'(o_i))^2} =(\mathbf Y-\mathbf A'^T\mathbf w')^T(\mathbf Y-\mathbf A'^T\mathbf w')$$

To minimise $TSE(LS,\mathbf w')$, we compute the null gradient : $\nabla_{\mathbf w'^*}TSE(LS,\mathbf w'^*) = 0$. Where $\mathbf w'*$ is the weights that minimise the $TSE$

## Introduction of a regularization term

Instead of choosing $\mathbf w'$ that minimise $TSE$, let minimise $\mathbf w'$ for given $\lambda > 0$:

$$TSE(LS, \lambda, \mathbf w')=(\mathbf Y-\mathbf A'^T\mathbf w')^T(\mathbf Y-\mathbf A'^T\mathbf w')+\lambda \mathbf w'^T \mathbf w$$

## Linear regression with kernels