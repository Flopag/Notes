# Q7 : Linear regression

> **Linear regression**: problem formulation, analytical solution, introduction of a regularization term (motivation and modified solution), linear regression with kernels.

## Problem formulation

The learning sample (LS) is composed of observations $o_i$

observation $o_i$ is composed of inputs $a_j(o_i) = a_j^i$ and an output $y^i$
- $\forall i = 1, ..., N$
- $\forall j = 1,...,n$

So, we can model LS with two matrix :
- The matrix of inputs $A=a_j^i$
- The matrix of outputs $Y = y^i$

![](attachments/Pasted%20image%2020231015165312.png)

Linear regression tries to approximate outputs by using :

$$\hat y(o) = w_0+\sum_{i=0}^n{w_i a_i(o)}$$

Must choose parameters $w_0, w_1, ..., w_n$ to fit well (supervised learning)

## Analytical solution

### Least mean square error solution

Principle : we look for $w'$ that minimise $TSE(LS,w')$ 

We pose $a_0(o)=1$ :
- $a'^i = (a_0^i,a_1^i , ..., a_n^i)^T$
- $w'=(w_0, w_1, ..., w_n)^T$
- $A'=(a'^1, a'^2, ..., a'^N)$

The squared error is :

$$SE(o_i, w')=(y^i-\hat y^i)² = (y^i, w'^Ta'^i)²$$

The total squared error is :

$$TSE(LS,w')=(y-A'^Tw')^T(y-A'^Tw')$$

## Introduction of a regularization term

## Linear regression with kernels