# Q9 : From single neuron models to multilayer perceptrons

> **From single neuron models to multilayer perceptrons**: single neuron models (batch vs online learning, hard and soft threshold unit, theoretical properties), multilayer perceptrons (definition, representation capacity, general principles of the training algorithm).

## Single neuron models

### Batch vs online learning

batch : samples are provided and processed together, and then to construct the model. So, the model is constructed by using the hole samples in one time

Online learning : samples are provided and processed one by one. So, the model is constructed by adding one sample by one sample (adaptative)

### Hard vs soft threshold unit

The input/output function $g(a)$ of such a device is computed by :

$$g(a) = f(w_0+\mathbf w^T \mathbf a)$$

Hard threshold (LTU) : threshold (hyperplane) that separate in a non continuous way. For example : $f() = sign()$

Soft threshold (STU) : threshold (hyperplane) that separate in a continuous way. For example : $f()=tanh()$

### Theoretical properties

Convergence of the perceptron learning algorithm :
- If LS is linearly separable (LS can be separated by a hyperplane) : converges in a finite number of steps.

Convergence of the online or batch gradient descent algorithm :
- if $\eta_i \rightarrow 0$ (slowly), and infinite number of steps, same solution
- if $f()$ linear, finds same solution as linear regression

## Multilayer perceptrons

### Definition

A multi layer perceptrons is multiple layers of neurons, with each layer fully connected to the next

Notations :
- $L$ number of layers
- $s_l$ $(1\leq l \leq L)$ is the number of neurons in layer $l$
- $a_i^{(l)}$ $(1< l \leq L, 1 \leq i \leq s_l)$ is the activation (output) of the neuron $i$ of the layer $l$ for an object $o$
- $f^{(l)}$ $(2\le l \leq L)$ is the activation function of layer $l$
- $w_{i,j}^{(l)}$ $(1\leq i \leq s_{l+1}, 1 \leq j \leq s_l)$ is the weight of the edge from neuron $j$ in layer $l$ to neuron $i$ in layer $l+1$
- $w_{i,0}^{(l)}$ $(1\leq i \leq s_{l+1})$ is the bias/intercept of neuron $i$ in layer $l+1$

Prediction (recursively) :

$$a_i^{(1)}(o) = a_i (o)$$

$$a_i^{(l+1)}(o)=f^{(l+1)}(w_{i,0}^{(l)}+\sum_{j=1}^{s_l}{w_{i,j}^{(l)}a_j^{(l)}(o)})$$

![](attachments/Pasted%20image%2020231022175842.png)

### Representation capacity

/

### General principles of the training algorithm

Main idea : finding the parameters $W$ that minimises the average loss over the training data :

$$W^* = \arg \max_W{\frac 1 N \sum_{o\in LS}{L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}}$$

with :
- $W$ the weights of all network
- $L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$ the loss function that compare the output layer prediction ($\mathbf g()$) and the true outputs ($\mathbf y()$)

We use the gradient descent to iteratively improve an initial value of $W$ :

$$\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$$