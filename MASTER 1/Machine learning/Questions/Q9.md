# Q9 : From single neuron models to multilayer perceptrons

> **From single neuron models to multilayer perceptrons**: single neuron models (batch vs online learning, hard and soft threshold unit, theoretical properties), multilayer perceptrons (definition, representation capacity, general principles of the training algorithm).

## Single neuron models

### Batch vs online learning

batch : samples are provided and processed together, and then to construct the model. So, the model is constructed by using the hole samples in one time

Online learning : samples are provided and processed one by one. So, the model is constructed by adding one sample by one sample (adaptative)

### Hard vs soft threshold unit

The input/output function $g(a)$ (= prediction) of such a device is computed by :

$$g(a) = f(w_0+\mathbf w^T \mathbf a) = f(\mathbf w'^T \mathbf a)$$

Where $\mathbf w'$ are the weighs parameters

Hard threshold (LTU) : threshold (hyperplane) that separate in a non continuous way. For example : $f() = sign()$

![](attachments/Pasted%20image%2020231227105336.png)

Soft threshold (STU) : threshold (hyperplane) that separate in a continuous way. For example : $f()=tanh()$

### Theoretical properties

Convergence of the perceptron learning algorithm :
- If LS is linearly separable (LS can be separated by a hyperplane) : converges in a finite number of steps. Otherwise: converge in a infinite number of steps (if $\eta_i \rightarrow 0$)

Convergence of the online or batch gradient descent algorithm :
- if $\eta_i \rightarrow 0$ (slowly), and infinite number of steps, same solution
- if $f()$ linear, finds same solution as linear regression

## Multilayer perceptrons

### Definition

A multi layer perceptrons is multiple layers of neurons, with each layer fully connected to the next. That permit to use several simple model into a complex one

Notations :
- $L$ number of layers (including input and output layers)
- $s_l$ $(1\leq l \leq L)$ is the number of neurons in layer $l$
- $a_i^{(l)}(o)$ $(1< l \leq L, 1 \leq i \leq s_l)$ is the activation (output) of the neuron $i$ of the layer $l$ for an object $o$
- $f^{(l)}$ $(2\le l \leq L)$ is the activation function of layer $l$
- $w_{i,j}^{(l)}$ $(1\leq i \leq s_{l+1}, 1 \leq j \leq s_l)$ is the weight of the edge from neuron $j$ in layer $l$ to neuron $i$ in layer $l+1$
- $w_{i,0}^{(l)}$ $(1\leq i \leq s_{l+1})$ is the bias/intercept of neuron $i$ in layer $l+1$

Prediction (recursively) :

$$a_i^{(1)}(o) = a_i (o)$$

$$a_i^{(l+1)}(o)=f^{(l+1)}(w_{i,0}^{(l)}+\sum_{j=1}^{s_l}{w_{i,j}^{(l)}a_j^{(l)}(o)})$$

![](attachments/Pasted%20image%2020231022175842.png)

### Representation capacity

I don't understand

### General principles of the training algorithm

The goal is to find the best fit of the parameters vector $\mathbf w'$

### Hard threshold unit

We define:
- Output encoded by $c(o_i) = \pm 1$ (binary classification)
- Initial weight vector: $\mathbf w'_0 = \mathbf 0$
- Object of the LS are considered as a cyclic.random sequence
- $o_i$ is the object considered at step $i$
- $c(o_i)$ is the class of the object $o_i$
- $\mathbf a(o_i)$ is the attributes of the object $o_i$

At each step, we update the hyperplane by using the following correction rule:

$$\mathbf w'_{i+1} = \mathbf w'_i+\eta_i[c(o_i)-g(\mathbf a(o_i))]\mathbf a'(o_i)$$

Where:
- $\eta_i>0$ is the learning rate
- $c(o_i)-g(\mathbf a(o_i))$ is $1$ if $o_i$ is incorrectly classified and is $0$ is correctly classified (say if the weights must be updated or not)
- $\mathbf a'(o_i)$ update the hyperplane to the right direction

![](attachments/Pasted%20image%2020231227111208.png)

The minus side of the hyper plane means that $g(\mathbf a(o_i)) = -1$ and the plus side means that $g(\mathbf a(o_i)) = 1$

### Soft threshold unit

For batch mode, we search the vector $\mathbf w'$ that minimize the square error (using its gradient):

$$TSE(LS, \mathbf w') = \sum_{o\in LS}{(g(\mathbf a(o)) - y(o))^2} = \sum_{o\in LS}{(f(\mathbf w'^T \mathbf a'(o)) - y(o))^2}$$

For online mode, we can do like the hard threshold unit but with a adapted correction rule:

$$\mathbf w'_{i+1} = \mathbf w'_i-\eta_i\nabla_{\mathbf w'}SE(o_i, \mathbf w'_i)$$

Where:
- $\eta > 0$ is the learning rate
- $SE(o, \mathbf w')$ is the contribution of object $o$ in $TSE(LS, \mathbf w')$

### Multilayer perceptrons

Main idea : finding the parameters $W$ that minimises the average loss over the training data :

$$W^* = \arg \max_W{\frac 1 N \sum_{o\in LS}{L(\mathbf g (\mathbf a (o);W),\mathbf y (o))}}$$

with :
- $W$ the weights of all network
- $L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$ the loss function that compare the output layer prediction ($\mathbf g()$) and the true outputs ($\mathbf y()$)

We use the gradient descent to iteratively improve an initial value of $W$ :

$$\frac{\partial}{\partial w_{i,j}^{(l)}}L(\mathbf g (\mathbf a (o);W),\mathbf y (o))$$