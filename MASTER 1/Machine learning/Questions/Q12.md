# Q12: Kernel methods

> **Kernel methods**: kernel definition, kernel trick, examples of kernels and of kernel methods. Discuss the interest of kernel methods in general. Kernel interpretation of trees and linear classification.

## Kernel definition

Let $U$ be a non empty set off object, then a positive kernel is a function $K(\bullet , \bullet)$ : $U \times U \rightarrow \Re$ Such that for all $N \in Natural$ and all $o_1,...,o_N\in U$ the $N \times N$ matrix $K:K_{i,j}=K(o_i,o_j)$ is symmetric and positive (semi)definite 

> For any positive kernel $K$ defined on $U$, there exists a scalar product space $V$ and a function $\phi(.):U\rightarrow V$,such that
>
>$$K(o,o')=\phi(o)\times\phi(o')$$
>
> where the operator $\times$ denotes the scalar product in $V$

## Kernel trick

for non linear boundary, we can remap the data into a new feature space where the boundary is linear

![](attachments/Pasted%20image%2020231226101309.png)

Lets consider a non-linear mapping $\phi$ to a new feature space $\phi(x)=[z_1, z_2, z_3]^T = [x^2_1, x_2^2, \sqrt 2 x_1 x_2]^T$

the dual problem becomes:

$$\max_\alpha W(\alpha) = \sum_{k=1}^N{\alpha_k}-\frac 1 2 \sum_{i, j=1}^N{\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)}$$

Instead of explicitly defining a mapping $\phi$, we can directly specify the dot product $\phi^T(x)\phi(x')$ and leave the mapping implicit

The kernel trick consist in replacing this dot product by a kernel:

$$\phi^T(x)\phi(x') = K(x,x')$$

## examples of kernel and of kernel methods

- The constant kernel: $K(o,o')=1$
- Linear kernel defined on numerical attributes: $K(o,o')=\mathbf a^T(o)\mathbf a(o')$
- Hamming kernel for discrete attributes: $K(o,o') = \sum_{i=1}^m \delta_{a_i(o),a_i(o')}$
- Text kernel: number of common substrings in $o$ and $o'$
- Combinations of kernels:
	- Sum of several kernels is also a kernel
	- Product of several kernels is also a kernel
	- Polynomial kernels: $\sum_{i=0}^n a_i(K(x,x'))^i$
- $K(x,x')=(x^Tx')^2$

![](attachments/Pasted%20image%2020231228170817.png)

## Interest of kernel

- We can work in very high dimentional spaces efficiently as soon as the inner product is cheap to compute
- Can apply classical algorithms on general, non-vectorial data types
- We can kernelize many algorithms

## Kernel interpretation

### Trees

### Linear classification