# Q8 : K-Nearest Neighbors methods

> **K-Nearest Neighbors methods**: 1-NN and k-NN algorithms, refinements, relations with tree-based, linear, and kernel methods.

## 1-NN

1-NN is supervised learning

### Principle

The prediction is the value of the nearest neighbour :

$$\hat y (o)= y(\arg\min_{o'\in LS}{d_a(o,o')})$$

with $d_a(o,o')=\sum_{i=1}^n(a_i(o)-a_i(o'))^2$, the distance between $o$ and $o'$

We can have weights:

$$d_a^w(o,o')=\sum_{i=1}^nw_i(a_i(o)-a_i(o'))^2$$

### Properties

- The model must store the learning sample ($n\times N$)
- To compute the distance, must compute $N$ distance (one for each $o$)
- For $N\rightarrow \infty$, 1-NN is sub-optimal (because of overlapping/mixing outputs $y$ $\rightarrow$ random choices)
	- If deterministic, optimal (because no overlapping/mixing)
- There is a strong dependency with the choice of attributes

## k-NN

### Principle

Same as 1-NN but take the mean of k nearest neighbour instead of 1 neighbour (1-NN is k-NN with k=1) :

$$\hat y(o) =k^{-1} \sum_{o'\in k \ nearest\ neighbours}{y(o')}$$

$k$ permit to handle hover/underfitting by choosing its value

### Properties

- For $N\rightarrow 0$, 1-NN is optimal

### Pros and cons

Pros:
- Very simple
- Can be adapted to any data type by changing the distance measure

Cons:
- Choosing a good distance measure is a hard problem
- Very sensitive to the presence of noisy variable
- Slow for testing

## Refinements

**Condensing and editing LS**

We can reduce the memory used in the model by saving only useful object. These steps can be followed :
1. Editing : remove useless objects
2. Condensing : keep only border objects

![](attachments/Pasted%20image%2020231017180724.png)

**Automatic tuning of the weight vector $w$**

**Parzen window and/or kernel methods**

$$\hat y(o) = \sum_{o'\in LS}{y(o')K(o,o')}$$

Where $K$ is a measure of similarity

## Relation with 

### tree-based and kernel

We can build the Kernel using a tree : 

$$K_T(o,o') = \left\{\begin{matrix} N_i^{-1} & if \ o,o' \ in \ same \ leaf\\ 0 & else\end{matrix}\right.$$

Where $N_i$ is the number of object in $i$

So, the approximation of the regression tree can be written as :

$$\hat y_T(o)=\sum_{o'\in LS}{y(o')K_T(o,o')}$$

The kernel can also be represented as a scalar :

$$K_T(o,o')=\mathbf{a^T_T}(o)\mathbf{a_T}(o')$$

with $\mathbf a_T(o) = (a_{L_1}(o), ..., a_{L_{|L|}}(o))^T$ where :
- $L_i$ is the leaf i
- $a_{L_i}(o)$ $=N_i^{-1/2}$ if $o$ is in $L_i$ and $=0$ else

### Linear

Lets consider a two-class classification problem. We define $y(o)=1$ if $c(o) = c_1$ and $y(o)=-1$ if $c(o) = c_2$

Construction of the classifier:
- Center of class 1 (mean of attributes on class 1): $\mathbf c_+ = N_+^{-1}\sum_{o'\in LS_+}\mathbf a(o')$
- Center of class 2 (mean of attributes on class 2): $\mathbf c_- = N_+^{-1}\sum_{o'\in LS_-}\mathbf a(o')$

The classifier become:

$$\hat y(o) = \left\{\begin{matrix} 1 & if \ d(\mathbf c_+,\mathbf a(o)) < d(\mathbf c_-,\mathbf a(o))\\ -1 & else\end{matrix}\right.$$