# Q8 : K-Nearest Neighbors methods

> **K-Nearest Neighbors methods**: 1-NN and k-NN algorithms, refinements, relations with tree-based, linear, and kernel methods.

## 1-NN

1-NN is supervised learning

### Principle

The prediction is the value of the nearest neighbour :

$$\hat y = y(\arg\min_{o'\in LS}{d_a(o,o')})$$

with $d_a(o,o')=\sum_{i=1}^n(a_i(o)-a_i(o'))^2$, the distance between $o$ and $o'$

### Properties

- The model must store the learning sample ($n\times N$)
- To compute the distance, must compute $N$ distance (one for each $o$)
- For $N\rightarrow 0$, 1-NN is sub-optimal (because of overlapping/mixing outputs $y$ $rightarrow$ random choices)
	- If deterministic, optimal (because no overlapping/mixing)
- There is a strong dependency with the choice of attributes

## k-NN

### Principle

Same as 1-NN but take the mean of k nearest neighbour instead of 1 neighbour (1-NN is k-NN with k=1) :

$$\hat y =k^{-1} \sum_{o'\in k-nearest-neighbours}{y(o')}$$

$k$ permit to handle hover/underfitting by choosing its value

### Properties

- For $N\rightarrow 0$, 1-NN is optimal

## Refinements

### Condensing and editing LS

We can reduce the memory used in the model by saving only useful object. These steps can be followed :
1. Condensing : remove useless objects
2. Editing : keep only border objects

![](attachments/Pasted%20image%2020231017180724.png)

### Relation with tree-based

We can build the Kernel using a tree : 

$$K_T(o,o') = \left\{\begin{matrix} N_i^{-1} & if \ o,o' \ in \ same \ leaf\\ 0 & else\end{matrix}\right.$$

with $N_i$, the number of object in $i$

So, the approximation of the regression tree can be written as :

$$\hat y_T(o)=\sum_{o'\in LS}{y(o')K_T(o,o')}$$

The kernel can be represented as a scalar :

$$K_T(o,o')=\mathbf{a^T_T}(o)\mathbf{a_T}(o')$$

with $\mathbf a_T(o) = (a_{L_1}(o), ..., a_{L_{|L|}}(o))^T$ where :
- $L_i$ is the leaf i
- $a_{L_i}(o)$ $=N_i^{-1/2}$ if $o$ is in $L_i$ and $=0$ else

### Linear

see slide 12 of "IML_Nearest_Neighbors_methods.pdf" (I have no time to do it now)