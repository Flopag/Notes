# Q13 : Ensemble methods

> **Ensemble methods**: motivation and algorithms (bagging, random forests, boosting, stackingâ€¦), ambiguity decomposition.

## Motivation

Combine the predictions of several models built with a learning algorithm. That permit to change the bias/variance trade-off but that destroys some features of the initial method

Two main families:
- Averaging techniques: grow several models independently and simply average their prediction. Mainly decrease variance
- Boosting techniques: grow several models sequentially. Mainly decrease bias

## Algorithms

### Bagging

(is an avaraging techniques and use "perturb and combine" algorithm because use bootstrap)

1. Draw $T$ bootstrap samples $\{B_1, B_2, ..., B_T\}$ from $LS$
2. Learn a model $\hat y_{B_i}$ from each $B_i$
3. Build the average model:
	- Regression: $\hat y_{ens}(\underline x) = \frac 1 T \sum_{i=1}^T{\hat y_{B_i}(\underline x)}$
	- Classification: $\hat y_{ens}(\underline x) = \frac 1 T majority\{\hat y_1(\underline x), ..., \hat y_T(\underline x)\}$

The variance is reduce but bias increase a bit because the effective size of a bootstrap sample is about 37% smaller than the original LS

$T$ must be the largest as possible

![](attachments/Pasted%20image%2020231229095423.png)

### Random forests

(is an averaging techniques and use "perturb and combine" algorithm because use bagging)

It combines bagging and random attribute subset selection: Build the tree from a bootstrap sample, but, Instead of choosing the best split among all attributes, select the best split among a random subset of $k$ attributes

There is a bias/variance trade-off with $k$:
- The smaller the $k$, the greater the reduction of variance
- The smaller the $k$, the higher the increase of bias

Random forest decrease computing times with respect to bagging since only a subset of all attributes is considered when splitting a node

### Boosting

Combine the output of many weak (= high bias) models to produce a powerful ensemble of models. The predictions of the models are combined through a weighted sum/vote

![](attachments/Pasted%20image%2020231229101219.png)

Regression:

$$\hat y (\underline x) = \sum_{i=1}^T \beta_i \hat y_i(\underline x)$$

Classification:

$$\hat y(\underline x)= weighted\_majority\{\hat y_1(\underline x), ..., \hat y_T(\underline x)\} \ \ given \ \ \{\beta_1,..., \beta_T\}$$

The generic boosting algorithm: find $\hat y (\underline x) = \sum_{i=1}^T \beta_i \hat y_i(\underline x)$ that minimizes $\hat y (\underline x) = \sum_{i=1}^N L(y_i, \hat y(\underline x_i))$
1. Initialize $\hat y(\underline x)=0$
2. For $t=1$ to $T$:
	1. Compute $(\beta_t, \hat y_t) = arg \min_{\beta, \hat y'} \sum_i L(y_i, \hat y(\underline x_i)+\beta \hat y'(\underline x_i))$
	2. Set $\hat y(\underline x) \leftarrow \hat y(\underline x)+\beta_t\hat y_t(\underline x)$

Adaboost ($L(y,y')=e^{-yy'}$): increase the weights of cases from the learning sample being misclassified by the last model
1. Initialize the weights: $w_i = \frac 1 N, i=1,...,N$
2. For $t=1$ to $T$:
	1. Build a model $\hat y_t(x)$ with the learning algorithm using weights $w_i$
	2. Compute the weighted error: $err_t=\frac{\sum_i w_i \ I(y_i\ne \hat y_i(x_i))}{\sum_i w_i}$
	3. Compute $\beta_t = log(\frac{1-err_t}{err_t})$
	4. Change the weights according to $w_i \leftarrow w_i e^{\beta_t \ I(y_i \ne \hat y_t(x_i))}$
	5. Normalize the weights such that $\sum_i w_i = 1$

Residual fitting ($L(y,y')=(y-y')^2$): boosting algorithm for regression
1. Initialize the initial model and initial residuals
	- $\hat y_0(x)=\frac 1 N \sum_i y_i$
	- $r_i = y_i (i=1,...,N)$
2. For $t=1$ to $T$
	1. For $i=1$ to $N$, compute the residuals $r_i \leftarrow r_i - \hat y_{t-1}(x_i)$
	2. Build a regression tree from the learning sample $\{(x_i,r_i):i=1,...,N\}$
3. Return the model $\hat y(x)=\hat y_0(x)+...+\hat y_t(x)$

Boosting reduces the bias but increase the variance

### Stacking

Learn a model that combine the models:
1. Let $A^t, t=0,...,T$ be $T+1$ learning algorithms
2. For $t=1,...,T$:
	1. Construct a model: $\hat y^t = A^t(LS)$
	2. Compute prediction: $\hat y _i^t=\hat y_i^t(x_i)$
3. Set $LS^0=\{(x_i^0, y_i)\}$ with $x_i^0=(\hat y_i^t)^T_{t=1}$
4. Return $\hat y=A^0(LS^0)$

### Others

See slides

## Ambiguity decomposition

![](attachments/Pasted%20image%2020231229110631.png)