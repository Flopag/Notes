# Q3 : Model evaluation methods

> **Model evaluation methods**: motivation, main approaches (resubstitution, test set, cross-validation, leave-one-out, bootstrap), advantages and drawbacks of each method, model selection by cross validation (small and large datasets), selection bias.

## Motivation

The model $\hat f_{LS}$ can be seen as a random variable. We must know which one is the best fit among multiple models.

Best fit = minimize the expected error (generalization error) = $Err_{LS} = E_{x,y}\{L(y,\hat f_{LS}(x))\}$

## Resubstitution

?

## K-fold Cross-validation

(compute the expected test error)

Randomly partition the dataset into k subsets. Then, choose one subset to be the test sample and the rest to be the learning sample. Get the means error of these predictions

Choosing the value of $k$:
- $k=N$ :
	- Unbiased: removing one object does not change much the size of LS
	- High variance: highly data set dependent
	- Slow: must train N models
- $k<N$:
	- Lower variance and faster: must train less models
	- Potentially biased: smaller LS and bigger TS (see graph bellow in test set method)

![](attachments/Pasted%20image%2020231228093355.png)

**Model selection:**

We use two stages of k-fold cross-validation. The first stage is used for assessment (estimation of the prediction error) of the final model. The second is used for model selection (get the best model) 

![](attachments/Pasted%20image%2020231228100741.png)

The estimation of the prediction error is important because making the prediction error on the second stage will leads to wrong result (ex: 10% on stage 2 and 50% on stage 1). Same for test set method.

## Leave-one-out

(compute the expected test error)

K-fold cross-validation with $k=N$

## Bootstrap

(compute the expected test error)

Bootstrap sample process:
1. Partition the dataset into multiple subset of same size
2. Make a new dataset (= bootstrap sample) of same size but with randomly chosen subset (with replacement)

![](attachments/Pasted%20image%2020231228093729.png)

Bootstrap error estimator method:
1. For $i=1$ to $B$
	1. Take a bootstrap sample $B_i$ from the dataset
	2. Learn a model $f_i$ on it
	3. Test the expected error with all subset that are not in the bootstrap sample ($â‰ƒ 37\%$, not arbitrary)
2. Average the expected error for all models $f_i$

## Test set method

(compute the conditional test error)

Split the dataset in two: learning set and a test set

![](attachments/Pasted%20image%2020231228095923.png)

The method first fit the model on the learning set, then, test is on the est set

It can be only used for big dataset because testing data on a small part of a small data set in not accurate

![](attachments/Pasted%20image%2020231227164644.png)

Model selection:
1. Partition the dataset into an LS, a VS, and a TS

![](attachments/Pasted%20image%2020231228100029.png)

2. Fit the models to compare on the leaning set, using different algorithms/complexity
3. Select the best one based on its performance on the validation set
4. Retrain this model on LS + VS
5. Test it on the test set (performance estimation)
6. Retrain this model on LS + VS + TS to get the final model