# Q14: Feature selection

> **Feature selection**: motivation, filter, embedded, and wrapper techniques (give examples of approaches in each family and discuss their main advantages and drawbacks), selection bias.

## motivation

Feature = input

Feature selection is a technique to reduce the number of features used by the learning algorithm

Advantages:
- Avoid over fitting and improve model performance
- Improve interpretability
- Provide faster and more cost-effective models
- Reduce overall computing times, if the feature selection technique is fast

Lets $Y$ denote the class variable and $V=\{X_1, ..., X_p\}$ the set of input variable

A feature $X_i$ is:
- Strongly relevant iff $P(Y|X_i,V\backslash X_i)\ne P(Y|V\backslash X_i)$
- Weakly relevant iif is not strongly relevant and $P(Y|X_i,S)\ne P(Y|S)$ for some subset $S \subset V$
- Irrelevent otherwise

A Markov Boundary for $Y$ is a subset $M \subseteq V$ of variables that is minimal and $P(Y|M,V\backslash M) = P(Y|M)$

Feature selection is often formulated as finding a Markov boundary $M$ for $Y$

## Filter

A priori selection of the variables: Associate a relevance score to each feature and remove the low-scoring ones

Univariate scoring: compute score by looking to only one variable (ex: decision tree)

Multivariate scoring: compute score by looking to multiple variables

![](attachments/Pasted%20image%2020231229114150.png)

Pros:
- Univariate is fast and scalable
- Independent of the supervised learning algorithm
Cons:
- It ignores the supervised learning algorithm
- Univariate scoring ignore feature dependencies
- Multivariate scoring is slower than univariate scoring

## Embedded

Feature selection is embedded in the learning algorithm

examples:
- Decision tree node splitting
- Absolute weights in a linear SVM model

Pros:
- Usually computationally efficient
- Well integrated within the learning algorithm
- Multivariate
Cons:
- Specific to the learning algorithm

## Wrapper techniques (ex and pro/con)

Try to find a subset of features that maximizes the quality of the model induced by the leaning algorithm. The quality is asssessed by cross-validation

Approach:
- Forward/Backward selection: add (resp. remove) the variable that most decrease (resp. less increases) the error
- Optimization by generic algorithms

### Recursive feature elimination

Assume that we have a learning algorithm that can rank the features:
1. iterate:
	1. Learn a model from the current feature set
	2. Rank the features with the model
	3. Remove the feature with the smallest ranking
2. Keep the feature set that yields the lowest cross-validation error

![](attachments/Pasted%20image%2020231229120138.png)

Pros:
- Custom-tailored to the learning algorithm
- Find interactions and remove redundant variables
Cons:
- Prone to overfitting: it is often easy to find a small subset of noisy features that discriminate perfectly the classes
- Expensive: we have to build a model for each subset of variables

## Selection bias

The error can be wrong (see slides) so, we use this:

1. Divide the learning sample into 10 fold
2. For $i=1$ to $10$:
	1. Remove the $i$-th fold from the learning sample
	2. Select the top 20 variables from the remaining objects
	3. Learn the model using the 20 variables and the remaining objects
	4. Test the model on the $i$-th fold