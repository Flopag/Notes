# Q2: Model evaluation criteria

> **Model evaluation criteria**: main evaluation criteria in classification (error rate, sensitivity/specificity, ROC curvesâ€¦) and in regression (squared error, correlation...), evaluation during prediction versus evaluation during training.

## Main evaluation criteria in classification

### Error rate

Contingency table:

![](attachments/Pasted%20image%2020231228102101.png)

We can compare model performance by looking to their error rate or accuracy. Limitations: this criterion does not convey any information about the error distribution across classes (may miss some usefull informations)

The error rate (proportion of wrong guess) is :

$$Error \ rate = \frac{FP+FN}{P+N}$$

The accuracy (proportion of well guess) is:

$$Accuracy = 1 - Error \ rate$$

### Sensitivity/specificity

Instead of error rate and accuracy, we can use sensitivity and specificity:

$$Sensitivity=\frac {TP} P$$

$$Specificity=1-\frac{FP}{N}$$

Sensitivity shows the proportion of true positive  in the positive set. Specificity shows the proportion of true negative in the negative set. That permit to know the error distribution across classes (use for medical diagnosis).

### ROC curves

We want to compare models using sensitivity and specification. To do it, we draw the ROC curves of each models and compare them. The domain is:

![](attachments/Pasted%20image%2020231228103959.png)

The pointed line means the random model and we want to be as far of it as possible

ROC construction process:
1. Order the instances from greater to lower value of score
2. Begin in 0,0
3. Positive means up and negative means left

![](attachments/Pasted%20image%2020231228105841.png)

The area under the ROC curve can be interpreted as the probability that two objects randomly drawn from the sample are well ordered by the model

![](attachments/Pasted%20image%2020231228110102.png)

### Other

There is more measures that can be useful given the context

## Main evaluation criteria in regression

### Squared error

$$\frac 1 N \sum_{i=1}^N{(y_i-\hat y_i)^2}$$

### Correlation

Pearson correlation:

$$\frac{\sum_i (y_i-\frac 1 N \sum_j y_j)(\hat y_i-\frac 1 N \sum_j \hat y_j)}{(N-1)s_ys_{\hat y}}$$

![](attachments/Pasted%20image%2020231228110858.png)

Spearman rank correlation:

$$1-\frac{6\sum_i d_i^2}{N(N^2-1)}$$

![](attachments/Pasted%20image%2020231228110907.png)

### Other

/

## Evaluating during prediction vs during training

Performance measures for training can be different from performance measures for testing. because:
- Algorithmic:
	- A differentiable measure is amenable to gradient optimisation
	- A decomposable measure is amenable to online training
- Overfitting:
	- For training, the loss function often incorporates a penalty term for model complexity, which is irrelevant at test time
	- Some measures are less prone to overfitting